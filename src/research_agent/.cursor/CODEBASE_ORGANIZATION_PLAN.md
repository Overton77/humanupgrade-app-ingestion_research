# Research Agent Codebase Organization Plan

> **ðŸ“‹ Living Documentation**: As we implement this system, we will create detailed `.cursor/rules/*.mdc` files and specifications that live on. This ensures each component has persistent, AI-accessible documentation that guides future development and maintenance.

## Executive Summary

This document outlines the **NEW PHASED APPROACH** for building the `research_agent` codebase. We are moving away from the entity-candidate-domain-catalog approach to a more flexible, conversation-first architecture.

### Implementation Phases (In Order)

1. **PHASE 1: Coordinator Agent** (Priority 1) - Interactive research plan builder with human-in-the-loop
2. **PHASE 2: Research Plan & Mission Orchestration** (Priority 2 - MOST CRITICAL) - Redesigned flexible execution
3. **PHASE 3: Entity Candidates Refactor** (Priority 3) - Lightweight candidate ranking tool  
4. **PHASE 4: Extraction & Storage** (Priority 4) - Knowledge graph ingestion pipeline

### Architecture Supports

1. **FastAPI Server Layer** (4 servers: Coordinator, Mission Control, Memory/Thread, Extraction)
2. **Distributed Task Execution** (Taskiq + RabbitMQ + Redis + MongoDB state)
3. **MongoDB Persistence** (Beanie ODM for all research artifacts + thread messages)
4. **Neo4j Knowledge Graph Ingestion** (via GraphQL client)
5. **LangGraph Memory System** (Semantic, Episodic, Procedural via PostgreSQL Store)
6. **Next.js Research Client** (Human-in-the-loop interface in `research_client/`)

**Primary Goal**: Build a flexible, human-guided research system where plans are created conversationally (Coordinator Agent) and executed flexibly (redesigned Mission Orchestration), with existing code refactored/reused where appropriate.

---

## Current State Analysis

### Existing Directory Structure

```
ingestion/src/research_agent/
â”œâ”€â”€ human_upgrade/                    # LEGACY MODULE TO ELIMINATE
â”‚   â”œâ”€â”€ graphs/                       # LangGraph state graphs
â”‚   â”‚   â”œâ”€â”€ entity_candidates_connected_graph.py
â”‚   â”‚   â”œâ”€â”€ research_plan_graph.py
â”‚   â”‚   â”œâ”€â”€ agent_instance_factory.py
â”‚   â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”‚   â””â”€â”€ langmem_manager.py
â”‚   â”‚   â””â”€â”€ nodes/                    # Graph node implementations
â”‚   â”œâ”€â”€ prompts/                      # Prompt templates
â”‚   â”‚   â”œâ”€â”€ candidates_prompts.py
â”‚   â”‚   â”œâ”€â”€ research_plan_prompts.py
â”‚   â”‚   â”œâ”€â”€ sub_agent_prompt_builders.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ structured_outputs/           # Pydantic output models
â”‚   â”‚   â”œâ”€â”€ candidates_outputs.py
â”‚   â”‚   â”œâ”€â”€ research_plans_outputs.py
â”‚   â”‚   â””â”€â”€ file_outputs.py
â”‚   â”œâ”€â”€ tools/                        # LangChain tools
â”‚   â”‚   â”œâ”€â”€ web_search_tools.py
â”‚   â”‚   â”œâ”€â”€ file_system_tools.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ utils/                        # Helper functions
â”‚   â”‚   â”œâ”€â”€ candidate_graph_helpers.py
â”‚   â”‚   â”œâ”€â”€ entity_slice_inputs.py
â”‚   â”‚   â”œâ”€â”€ formatting.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ persistence/                  # Checkpointer + store
â”‚   â”‚   â””â”€â”€ checkpointer_and_store.py
â”‚   â”œâ”€â”€ base_models.py               # LLM model definitions
â”‚   â””â”€â”€ logger.py
â”œâ”€â”€ mission_queue/                    # NEW: DAG execution (keep)
â”‚   â”œâ”€â”€ mission_dag_builder.py
â”‚   â”œâ”€â”€ scheduler_in_memory.py
â”‚   â”œâ”€â”€ worker.py
â”‚   â””â”€â”€ schemas.py
â”œâ”€â”€ models/                           # NEW: MongoDB Beanie models (keep)
â”‚   â””â”€â”€ mongo/
â”‚       â”œâ”€â”€ research/
â”‚       â”œâ”€â”€ candidates/
â”‚       â”œâ”€â”€ entities/
â”‚       â””â”€â”€ domains/
â”œâ”€â”€ services/                         # NEW: MongoDB services (keep)
â”‚   â””â”€â”€ mongo/
â”‚       â”œâ”€â”€ candidates/
â”‚       â””â”€â”€ research/
â”œâ”€â”€ infrastructure/                   # NEW: Infrastructure (keep)
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â””â”€â”€ mongo/
â”‚   â”œâ”€â”€ document_processing/
â”‚   â””â”€â”€ embeddings/
â”œâ”€â”€ clients/                          # GraphQL, LangSmith clients (keep)
â”œâ”€â”€ agent_tools/                      # Legacy tools (needs refactor)
â””â”€â”€ server.py                         # Prototype FastAPI server (needs expansion)
```

### Problems with Current Structure

1. **`human_upgrade/` is a confusing legacy name** (does not reflect biotech research purpose)
2. **Mixed concerns** (graphs, prompts, tools, utils all in one module)
3. **No clear server organization** (single `server.py` prototype)
4. **Duplicate tool definitions** (`agent_tools/` vs `human_upgrade/tools/`)
5. **Inconsistent naming** (`human_upgrade.structured_outputs.research_plans_outputs` is verbose)
6. **No clear API layer** (no separation between internal models and API schemas)
7. **Memory module buried** (`human_upgrade/graphs/memory/` should be top-level)

---

## Proposed New Structure

### Overview: 5-Layer Architecture

```
research_agent/
â”œâ”€â”€ api/                    # FastAPI servers (Layer 1: External Interface)
â”œâ”€â”€ graphs/                 # LangGraph orchestration (Layer 2: Research Logic)
â”œâ”€â”€ agents/                 # Worker agent implementations (Layer 3: Execution)
â”œâ”€â”€ services/               # Business logic + DB operations (Layer 4: Services)
â”œâ”€â”€ infrastructure/         # External integrations (Layer 5: Infrastructure)
â”œâ”€â”€ models/                 # Data models (cross-layer)
â”œâ”€â”€ memory/                 # LangGraph Store + LangMem (cross-layer)
â””â”€â”€ shared/                 # Shared utilities (cross-layer)
```

---

## Detailed New Structure

```
ingestion/src/research_agent/
â”‚
â”œâ”€â”€ api/                                      # Layer 1: FastAPI Servers
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ common/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dependencies.py               # Shared FastAPI dependencies
â”‚   â”‚   â”œâ”€â”€ middleware.py                 # CORS, auth, logging
â”‚   â”‚   â”œâ”€â”€ exceptions.py                 # Custom exception handlers
â”‚   â”‚   â””â”€â”€ responses.py                  # Standard response schemas
â”‚   â”‚
â”‚   â”œâ”€â”€ coordinator/                      # Server 1: Coordinator Agent API (PHASE 1 - NEW)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py                       # FastAPI app
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ threads.py                # POST/GET /coordinator/threads/*
â”‚   â”‚   â”‚   â”œâ”€â”€ checkpoints.py            # POST /coordinator/checkpoints/{id}/approve
â”‚   â”‚   â”‚   â””â”€â”€ health.py                 # GET /health, /ready
â”‚   â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ threads.py                # Thread request/response models
â”‚   â”‚   â”‚   â””â”€â”€ checkpoints.py            # Checkpoint approval models
â”‚   â”‚   â””â”€â”€ websockets/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ coordinator_stream.py     # WebSocket streaming for Coordinator
â”‚   â”‚
â”‚   â”œâ”€â”€ mission_control/                  # Server 2: Mission Control API (PHASE 2 - ENHANCED)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ missions.py               # POST/GET /missions/*
â”‚   â”‚   â”‚   â”œâ”€â”€ runs.py                   # GET/POST /missions/runs/*
â”‚   â”‚   â”‚   â”œâ”€â”€ tasks.py                  # GET /tasks/{task_id}
â”‚   â”‚   â”‚   â””â”€â”€ plans.py                  # GET/POST /plans/*
â”‚   â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ missions.py               # MissionCreate/Read/Update
â”‚   â”‚   â”‚   â”œâ”€â”€ runs.py                   # RunStatus/Progress
â”‚   â”‚   â”‚   â””â”€â”€ tasks.py                  # TaskDefinition/Status
â”‚   â”‚   â””â”€â”€ services/                     # Mission orchestration logic
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ mission_orchestrator.py   # Build DAG, enqueue tasks
â”‚   â”‚       â””â”€â”€ task_monitor.py           # Monitor task progress
â”‚   â”‚
â”‚   â”œâ”€â”€ memory_and_threads/               # Server 3: Memory & Thread API
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ threads.py                # GET/POST /threads/*
â”‚   â”‚   â”‚   â”œâ”€â”€ checkpoints.py            # GET/POST /threads/{id}/checkpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ memory.py                 # GET/POST /store/memories/*
â”‚   â”‚   â”‚   â””â”€â”€ recall.py                 # POST /store/recall (semantic search)
â”‚   â”‚   â””â”€â”€ schemas/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ threads.py
â”‚   â”‚       â”œâ”€â”€ checkpoints.py
â”‚   â”‚       â””â”€â”€ memory.py
â”‚   â”‚
â”‚   â””â”€â”€ mongodb_models/                   # Server 4: MongoDB Model API (CRUD)
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ main.py
â”‚       â”œâ”€â”€ routes/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ research_plans.py         # CRUD for ResearchMissionPlanDoc
â”‚       â”‚   â”œâ”€â”€ research_runs.py          # CRUD for ResearchRunDoc
â”‚       â”‚   â”œâ”€â”€ candidates.py             # CRUD for candidate docs
â”‚       â”‚   â””â”€â”€ entities.py               # CRUD for entity docs
â”‚       â””â”€â”€ schemas/
â”‚           â”œâ”€â”€ __init__.py
â”‚           â””â”€â”€ mongo_models.py           # API-friendly Pydantic schemas
â”‚
â”œâ”€â”€ graphs/                                # Layer 2: LangGraph Orchestration
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ coordinator/                      # PHASE 1: Coordinator Agent Graph (NEW)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ graph.py                      # Main coordinator graph builder
â”‚   â”‚   â”œâ”€â”€ nodes/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ understand_goals.py       # Extract research goals from user
â”‚   â”‚   â”‚   â”œâ”€â”€ query_knowledge.py        # Query existing KG entities
â”‚   â”‚   â”‚   â”œâ”€â”€ query_past_research.py    # Query similar past runs
â”‚   â”‚   â”‚   â”œâ”€â”€ build_scope.py            # Build research scope
â”‚   â”‚   â”‚   â”œâ”€â”€ scope_checkpoint.py       # Human approval checkpoint #1
â”‚   â”‚   â”‚   â”œâ”€â”€ suggest_strategies.py     # Suggest research strategies
â”‚   â”‚   â”‚   â”œâ”€â”€ build_stages.py           # Build research stages
â”‚   â”‚   â”‚   â”œâ”€â”€ allocate_agents.py        # Allocate agent types
â”‚   â”‚   â”‚   â”œâ”€â”€ final_plan_checkpoint.py  # Human approval checkpoint #2
â”‚   â”‚   â”‚   â””â”€â”€ save_and_emit.py          # Save plan, emit to mission queue
â”‚   â”‚   â”œâ”€â”€ state.py                      # CoordinatorAgentState
â”‚   â”‚   â””â”€â”€ helpers.py                    # Coordinator-specific helpers
â”‚   â”‚
â”‚   â”œâ”€â”€ candidate_exploration/            # PHASE 3: Candidate Exploration (REFACTORED)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ graph.py                      # Lightweight exploration graph
â”‚   â”‚   â”œâ”€â”€ nodes/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ quick_extraction.py       # Fast entity extraction
â”‚   â”‚   â”‚   â”œâ”€â”€ relevance_ranking.py      # Rank by relevance
â”‚   â”‚   â”‚   â”œâ”€â”€ novelty_check.py          # Check against KG
â”‚   â”‚   â”‚   â””â”€â”€ completeness_estimate.py  # Estimate researchability
â”‚   â”‚   â”œâ”€â”€ state.py
â”‚   â”‚   â””â”€â”€ helpers.py
â”‚   â”‚
â”‚   â”œâ”€â”€ research_planning/                # PHASE 2: Research Planning (REDESIGNED)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ graph.py                      # Flexible research plan graph
â”‚   â”‚   â”œâ”€â”€ nodes/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ validate_plan.py          # Validate plan structure
â”‚   â”‚   â”‚   â”œâ”€â”€ optimize_dependencies.py  # Optimize stage dependencies
â”‚   â”‚   â”‚   â””â”€â”€ prepare_for_execution.py  # Final prep before execution
â”‚   â”‚   â”œâ”€â”€ state.py
â”‚   â”‚   â””â”€â”€ helpers.py
â”‚   â”‚
â”‚   â”œâ”€â”€ entity_extraction/                # PHASE 4: Extraction Graph (TO BE IMPLEMENTED)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ graph.py
â”‚   â”‚   â”œâ”€â”€ nodes/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ parse_reports.py
â”‚   â”‚   â”‚   â”œâ”€â”€ extract_entities.py
â”‚   â”‚   â”‚   â”œâ”€â”€ extract_relationships.py
â”‚   â”‚   â”‚   â””â”€â”€ neo4j_ingestion.py
â”‚   â”‚   â””â”€â”€ state.py
â”‚   â”‚
â”‚   â””â”€â”€ common/                           # Shared graph utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ persistence_nodes.py          # Generic Beanie persistence nodes
â”‚       â”œâ”€â”€ checkpointing.py              # Checkpoint + store config
â”‚       â””â”€â”€ error_handling.py             # Graph error recovery
â”‚
â”œâ”€â”€ agents/                                # Layer 3: Worker Agent Execution
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ factory/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ builder.py                    # build_worker_agent()
â”‚   â”‚   â”œâ”€â”€ runner.py                     # run_worker_once()
â”‚   â”‚   â””â”€â”€ middleware.py                 # Summarization, dynamic prompts
â”‚   â”‚
â”‚   â”œâ”€â”€ types/                            # Agent type implementations
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ business_identity.py          # BusinessIdentityAndLeadershipAgent
â”‚   â”‚   â”œâ”€â”€ person_bio.py                 # PersonBioAndAffiliationsAgent
â”‚   â”‚   â”œâ”€â”€ ecosystem_mapper.py
â”‚   â”‚   â”œâ”€â”€ product_spec.py
â”‚   â”‚   â”œâ”€â”€ claims_extractor.py
â”‚   â”‚   â”œâ”€â”€ case_study_harvest.py
â”‚   â”‚   â””â”€â”€ ...                           # All 11 agent types
â”‚   â”‚
â”‚   â”œâ”€â”€ state/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ worker_agent_state.py         # WorkerAgentState TypedDict
â”‚   â”‚
â”‚   â”œâ”€â”€ prompts/                          # Agent prompt builders
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ initial/                      # Initial system prompts
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ business_identity.py
â”‚   â”‚   â”‚   â”œâ”€â”€ person_bio.py
â”‚   â”‚   â”‚   â””â”€â”€ generic.py
â”‚   â”‚   â”œâ”€â”€ reminder/                     # Reminder prompts (w/ telemetry)
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ business_identity.py
â”‚   â”‚   â”‚   â””â”€â”€ generic.py
â”‚   â”‚   â””â”€â”€ final_synthesis/             # Final report synthesis prompts
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ business_identity.py
â”‚   â”‚       â””â”€â”€ generic.py
â”‚   â”‚
â”‚   â””â”€â”€ tools/                            # Agent tool configurations
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ tool_registry.py              # Central tool registry
â”‚       â”œâ”€â”€ default_tool_maps.py          # Default tools per agent type
â”‚       â””â”€â”€ tool_selection.py             # Dynamic tool selection logic
â”‚
â”œâ”€â”€ orchestration/                         # Layer 3.5: Mission DAG Execution
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dag/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ builder.py                    # build_mission_dag()
â”‚   â”‚   â”œâ”€â”€ schemas.py                    # MissionDAG, TaskDefinition
â”‚   â”‚   â””â”€â”€ task_ids.py                   # Deterministic task ID builders
â”‚   â”‚
â”‚   â”œâ”€â”€ scheduler/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ in_memory.py                  # In-memory scheduler (MVP)
â”‚   â”‚   â”œâ”€â”€ mongo_backed.py               # Future: Mongo-backed scheduler
â”‚   â”‚   â””â”€â”€ events.py                     # Event schemas (TASK_SUCCEEDED, etc.)
â”‚   â”‚
â”‚   â”œâ”€â”€ workers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ taskiq_worker.py              # Taskiq worker implementation
â”‚   â”‚   â”œâ”€â”€ handlers/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ instance_run.py           # Handle INSTANCE_RUN task
â”‚   â”‚   â”‚   â””â”€â”€ substage_reduce.py        # Handle SUBSTAGE_REDUCE task
â”‚   â”‚   â””â”€â”€ redis_consumer.py             # Redis stream consumer
â”‚   â”‚
â”‚   â””â”€â”€ queue/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ taskiq_broker.py              # Taskiq broker setup
â”‚       â””â”€â”€ redis_streams.py              # Redis stream utilities
â”‚
â”œâ”€â”€ services/                              # Layer 4: Business Logic + DB Operations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ mongo/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ research/                     # Research plan/run services
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ plan_service.py           # CRUD + queries for plans
â”‚   â”‚   â”‚   â””â”€â”€ run_service.py            # CRUD + queries for runs
â”‚   â”‚   â”œâ”€â”€ candidates/                   # Candidate discovery services
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ seed_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ official_sources_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ domain_catalog_service.py
â”‚   â”‚   â”‚   â””â”€â”€ connected_candidates_service.py
â”‚   â”‚   â”œâ”€â”€ entities/                     # Entity services
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ candidate_entity_service.py
â”‚   â”‚   â”‚   â””â”€â”€ dedupe_group_service.py
â”‚   â”‚   â””â”€â”€ common/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ base_service.py           # Generic CRUD operations
â”‚   â”‚
â”‚   â”œâ”€â”€ neo4j/                            # Future: Neo4j services
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ entity_ingestion_service.py   # Ingest entities to Neo4j
â”‚   â”‚   â””â”€â”€ relationship_service.py       # Manage relationships
â”‚   â”‚
â”‚   â””â”€â”€ graphql/                          # GraphQL client services
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ client.py                     # Ariadne-generated client
â”‚       â””â”€â”€ mutations.py                  # Common mutation builders
â”‚
â”œâ”€â”€ memory/                                # Cross-Layer: Memory System
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ langmem/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ manager.py                    # LangMem SDK wrapper
â”‚   â”‚   â”œâ”€â”€ schemas.py                    # Memory schemas (Semantic, Episodic, etc.)
â”‚   â”‚   â”œâ”€â”€ namespaces.py                 # Namespace routing logic
â”‚   â”‚   â””â”€â”€ extraction.py                 # Memory extraction workflows
â”‚   â”‚
â”‚   â”œâ”€â”€ store/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ checkpointer.py               # LangGraph checkpointer config
â”‚   â”‚   â”œâ”€â”€ postgres_store.py             # AsyncPostgresStore config
â”‚   â”‚   â””â”€â”€ recall.py                     # Memory recall utilities
â”‚   â”‚
â”‚   â””â”€â”€ tools/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ memory_tools.py               # LangChain tools for memory recall
â”‚
â”œâ”€â”€ models/                                # Cross-Layer: Data Models
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ enums.py                      # Shared enums
â”‚   â”‚   â””â”€â”€ base_models.py                # Base Pydantic models
â”‚   â”‚
â”‚   â”œâ”€â”€ mongo/                            # MongoDB Beanie models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ coordinator/                  # PHASE 1: Coordinator Models (NEW)
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ docs/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ coordinator_threads.py    # Conversation threads
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ coordinator_checkpoints.py # Human approval checkpoints
â”‚   â”‚   â”‚   â””â”€â”€ embedded/
â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚       â”œâ”€â”€ research_goals.py         # Structured research goals
â”‚   â”‚   â”‚       â””â”€â”€ research_scope.py         # Research scope model
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ research/                     # PHASE 2: Research Models (REDESIGNED)
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ docs/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ research_mission_plans.py # Flexible research plans
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ research_runs.py          # Mission execution tracking
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ agent_instance_outputs.py # Agent instance outputs (NEW)
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ substage_outputs.py       # Sub-stage aggregated outputs (NEW)
â”‚   â”‚   â”‚   â”œâ”€â”€ embedded/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ research_objectives.py    # ResearchObjective model (NEW)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ agent_instance_plan.py    # AgentInstancePlan model (NEW)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ sub_stage.py              # SubStage model (NEW)
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ stage.py                  # Stage model (NEW)
â”‚   â”‚   â”‚   â””â”€â”€ enums.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ candidates/                   # PHASE 3: Candidates (SIMPLIFIED)
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ docs/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ candidate_explorations.py # Exploration results (NEW)
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ranked_candidates.py      # Ranked candidates (NEW)
â”‚   â”‚   â”‚   â””â”€â”€ embedded/
â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚       â””â”€â”€ ranked_candidate.py       # Single ranked candidate
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ extraction/                   # PHASE 4: Extraction Models (NEW)
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ docs/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ extraction_runs.py        # Extraction pipeline runs
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ extracted_entities.py     # Pre-KG extracted entities
â”‚   â”‚   â”‚   â””â”€â”€ embedded/
â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚       â”œâ”€â”€ organization_extracted.py
â”‚   â”‚   â”‚       â”œâ”€â”€ person_extracted.py
â”‚   â”‚   â”‚       â”œâ”€â”€ product_extracted.py
â”‚   â”‚   â”‚       â””â”€â”€ compound_extracted.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ entities/                     # Keep for backward compatibility
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ docs/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ candidate_entities.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ dedupe_groups.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ candidate_runs.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ artifacts.py
â”‚   â”‚   â”‚   â””â”€â”€ embedded/
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ domains/                      # DEPRECATED (keep for backward compat)
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ docs/
â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â””â”€â”€ domain_catalog_sets.py
â”‚   â”‚       â””â”€â”€ embedded/
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                              # API schemas (FastAPI request/response)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ common.py                     # Common API schemas
â”‚   â”‚   â”œâ”€â”€ graph_execution.py
â”‚   â”‚   â”œâ”€â”€ mission_control.py
â”‚   â”‚   â””â”€â”€ memory.py
â”‚   â”‚
â”‚   â””â”€â”€ graph/                            # Graph-specific models
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ candidates.py                 # Entity discovery structured outputs
â”‚       â”œâ”€â”€ research_plans.py             # Research planning structured outputs
â”‚       â”œâ”€â”€ agent_plans.py                # Agent instance plan models
â”‚       â””â”€â”€ slicing.py                    # Slicing models
â”‚
â”œâ”€â”€ tools/                                 # Cross-Layer: LangChain Tools
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ web/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ tavily_search.py              # Tavily search tool
â”‚   â”‚   â”œâ”€â”€ tavily_extract.py             # Tavily extract tool
â”‚   â”‚   â”œâ”€â”€ exa_search.py                 # Exa search tool
â”‚   â”‚   â””â”€â”€ wikipedia.py                  # Wikipedia tool
â”‚   â”‚
â”‚   â”œâ”€â”€ graphql/                          # PHASE 1: GraphQL Query Tools (NEW)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ query_entities.py             # Query existing entities in KG
â”‚   â”‚   â”œâ”€â”€ get_entity_details.py         # Get full entity details
â”‚   â”‚   â””â”€â”€ search_by_type.py             # Search entities by type
â”‚   â”‚
â”‚   â”œâ”€â”€ research_history/                 # PHASE 1: Research History Tools (NEW)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ query_past_runs.py            # Query similar past research
â”‚   â”‚   â”œâ”€â”€ get_run_summary.py            # Get run summary/outcomes
â”‚   â”‚   â””â”€â”€ get_effective_agents.py       # Get agent types that worked
â”‚   â”‚
â”‚   â”œâ”€â”€ candidate_exploration/            # PHASE 3: Candidate Tools (NEW)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ explore_and_rank.py           # Explore and rank candidates tool
â”‚   â”‚
â”‚   â”œâ”€â”€ browser/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ playwright_browser.py         # Playwright browser tool
â”‚   â”‚
â”‚   â”œâ”€â”€ filesystem/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ read_file.py
â”‚   â”‚   â”œâ”€â”€ write_file.py
â”‚   â”‚   â””â”€â”€ workspace_helpers.py          # Workspace path helpers
â”‚   â”‚
â”‚   â”œâ”€â”€ scholarly/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ pubmed.py                     # PubMed tool
â”‚   â”‚   â”œâ”€â”€ semantic_scholar.py           # Semantic Scholar tool
â”‚   â”‚   â””â”€â”€ clinical_trials.py            # ClinicalTrials.gov tool
â”‚   â”‚
â”‚   â”œâ”€â”€ context/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ summarize.py                  # Context summarization tool
â”‚   â”‚
â”‚   â””â”€â”€ registry.py                       # Central tool registry
â”‚
â”œâ”€â”€ prompts/                               # Cross-Layer: Prompt Templates
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ graphs/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ entity_discovery/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ seed_extraction.py
â”‚   â”‚   â”‚   â”œâ”€â”€ official_sources.py
â”‚   â”‚   â”‚   â”œâ”€â”€ domain_catalogs.py
â”‚   â”‚   â”‚   â””â”€â”€ candidate_slices.py
â”‚   â”‚   â””â”€â”€ research_planning/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ initial_plan.py
â”‚   â”‚       â”œâ”€â”€ source_expansion.py
â”‚   â”‚       â””â”€â”€ attach_sources.py
â”‚   â”‚
â”‚   â””â”€â”€ agents/                           # Agent prompts (moved to agents/prompts/)
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ infrastructure/                        # Layer 5: External Integrations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ mongo/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ base_client.py            # PyMongo client
â”‚   â”‚   â”‚   â”œâ”€â”€ biotech_research_db_beanie.py
â”‚   â”‚   â”‚   â””â”€â”€ connection_manager.py
â”‚   â”‚   â”œâ”€â”€ s3/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ client.py                 # S3 client
â”‚   â”‚   â”‚   â””â”€â”€ artifact_storage.py       # Store reports/transcripts
â”‚   â”‚   â””â”€â”€ redis/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ client.py                 # Redis client
â”‚   â”‚       â””â”€â”€ cache.py                  # Caching utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ document_processing/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ docling_processor.py          # Docling PDF processing
â”‚   â”‚   â””â”€â”€ chunking.py                   # Text chunking
â”‚   â”‚
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ openai_embeddings.py          # OpenAI embeddings
â”‚   â”‚   â””â”€â”€ batch_embeddings.py           # Batch embedding generation
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ model_registry.py             # gpt_5_mini, gpt_4_1, etc.
â”‚   â”‚   â””â”€â”€ token_counting.py             # Token counting utilities
â”‚   â”‚
â”‚   â””â”€â”€ queue/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ taskiq_broker.py              # Taskiq broker setup
â”‚       â””â”€â”€ rabbitmq_client.py            # RabbitMQ client
â”‚
â”œâ”€â”€ shared/                                # Cross-Layer: Shared Utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ artifacts.py                      # Artifact saving (JSON, text)
â”‚   â”œâ”€â”€ datetime_helpers.py               # UTC now, formatting
â”‚   â”œâ”€â”€ dedupe.py                         # Deduplication helpers
â”‚   â”œâ”€â”€ formatting.py                     # Prompt formatting
â”‚   â”œâ”€â”€ validation.py                     # Input validation
â”‚   â”œâ”€â”€ logging_utils.py                  # Logging configuration
â”‚   â””â”€â”€ constants.py                      # Shared constants
â”‚
â”œâ”€â”€ clients/                               # External API clients (keep as is)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ graphql_client.py                 # Ariadne-generated GraphQL client
â”‚   â”œâ”€â”€ langsmith_client.py               # LangSmith client
â”‚   â””â”€â”€ async_tavily_client.py            # Async Tavily client
â”‚
â”œâ”€â”€ config/                                # Configuration
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py                       # Pydantic settings (env vars)
â”‚   â””â”€â”€ environments/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ development.py
â”‚       â”œâ”€â”€ staging.py
â”‚       â””â”€â”€ production.py
â”‚
â”œâ”€â”€ scripts/                               # CLI scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ init_db.py                        # Initialize MongoDB indexes
â”‚   â”œâ”€â”€ run_discovery.py                  # Run entity discovery graph
â”‚   â”œâ”€â”€ run_planner.py                    # Run research plan graph
â”‚   â”œâ”€â”€ run_scheduler.py                  # Run scheduler
â”‚   â”œâ”€â”€ run_worker.py                     # Run worker
â”‚   â””â”€â”€ test_data/                        # Test data for scripts
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ one_thousand_roads.py
â”‚
â”œâ”€â”€ tests/                                 # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ graphs/
â”‚   â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ tools/
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ test_entity_discovery_graph.py
â”‚   â”‚   â”œâ”€â”€ test_research_plan_graph.py
â”‚   â”‚   â””â”€â”€ test_mission_dag_execution.py
â”‚   â””â”€â”€ fixtures/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ mongo_fixtures.py
â”‚       â””â”€â”€ graph_fixtures.py
â”‚
â”œâ”€â”€ .cursor/                               # Cursor AI context
â”‚   â”œâ”€â”€ AGENTS.md
â”‚   â”œâ”€â”€ CODEBASE_ORGANIZATION_PLAN.md
â”‚   â”œâ”€â”€ PROJECT_GOALS_AGENT.md
â”‚   â””â”€â”€ text_diagrams/
â”‚
â”œâ”€â”€ __init__.py
â””â”€â”€ README.md
```

---

## NEW Implementation Strategy (Phased Approach)

> **Critical Change**: We are NOT migrating the old structure. We are building NEW components first (Phases 1-4), then refactoring existing code to integrate.

---

## PHASE 1: Coordinator Agent (Week 1)

**Goal**: Build interactive research plan builder with human-in-the-loop

### Step 1: Create Coordinator Directory Structure

```bash
# Phase 1 directories
mkdir -p api/coordinator/{routes,schemas,websockets}
mkdir -p graphs/coordinator/nodes
mkdir -p models/mongo/coordinator/{docs,embedded}
mkdir -p tools/{graphql,research_history}
mkdir -p tests/unit/coordinator
mkdir -p tests/integration/coordinator
```

### Step 2: Create MongoDB Models

```python
# models/mongo/coordinator/docs/coordinator_threads.py
class CoordinatorThreadDoc(Document):
    thread_id: str
    user_id: Optional[str]
    status: str  # "active" | "scope_approved" | "plan_approved"
    initial_query: str
    messages: List[Dict]  # Serialized BaseMessage
    # ... (see AGENTS.md for full spec)

# models/mongo/coordinator/docs/coordinator_checkpoints.py
class CoordinatorCheckpointDoc(Document):
    checkpoint_id: str
    thread_id: str
    checkpoint_type: str  # "scope_approval" | "final_plan_approval"
    state_snapshot: Dict
    approved: Optional[bool]
    # ... (see AGENTS.md for full spec)
```

### Step 3: Create GraphQL Query Tools

```python
# tools/graphql/query_entities.py
class QueryExistingEntitiesTool(BaseTool):
    """Query existing entities in the knowledge graph."""
    name = "query_existing_entities"
    description = "Search for entities already in the knowledge graph"
    
    async def _arun(self, query: str) -> str:
        # Call GraphQL API to search entities
        pass

# tools/graphql/get_entity_details.py
# tools/graphql/search_by_type.py
```

### Step 4: Create Research History Tools

```python
# tools/research_history/query_past_runs.py
class QueryPastResearchRunsTool(BaseTool):
    """Query similar past research missions."""
    name = "query_past_research_runs"
    description = "Find similar past research missions and their outcomes"
    
    async def _arun(self, query: str) -> str:
        # Query ResearchRunDoc collection
        # Return summaries of similar runs
        pass

# tools/research_history/get_run_summary.py
# tools/research_history/get_effective_agents.py
```

### Step 5: Build Coordinator LangGraph

```python
# graphs/coordinator/graph.py
from langgraph.graph import StateGraph, START, END

def build_coordinator_agent_graph():
    """Build the Coordinator Agent LangGraph."""
    
    builder = StateGraph(CoordinatorAgentState)
    
    # Nodes
    builder.add_node("understand_goals", understand_goals_node)
    builder.add_node("query_knowledge", query_knowledge_node)
    builder.add_node("build_scope", build_scope_node)
    builder.add_node("scope_checkpoint", scope_checkpoint_node)  # interrupt()
    builder.add_node("suggest_strategies", suggest_strategies_node)
    builder.add_node("build_stages", build_stages_node)
    builder.add_node("allocate_agents", allocate_agents_node)
    builder.add_node("final_plan_checkpoint", final_plan_checkpoint_node)  # interrupt()
    builder.add_node("save_and_emit", save_and_emit_node)
    
    # Edges
    builder.add_edge(START, "understand_goals")
    builder.add_edge("understand_goals", "query_knowledge")
    builder.add_edge("query_knowledge", "build_scope")
    builder.add_edge("build_scope", "scope_checkpoint")
    # ... (full flow in AGENTS.md)
    
    return builder.compile(
        checkpointer=get_postgres_checkpointer(),
        interrupt_before=["scope_checkpoint", "final_plan_checkpoint"]
    )
```

### Step 6: Create FastAPI Routes

```python
# api/coordinator/routes/threads.py
@router.post("/coordinator/threads")
async def create_coordinator_thread(request: CreateThreadRequest):
    """Start a new Coordinator Agent conversation."""
    # Create thread in MongoDB
    # Invoke graph with initial message
    # Return thread_id
    pass

@router.post("/coordinator/threads/{thread_id}/messages")
async def send_message(thread_id: str, message: SendMessageRequest):
    """Send a message to the Coordinator Agent."""
    # Append message to thread
    # Invoke graph
    # Return response
    pass

# api/coordinator/routes/checkpoints.py
@router.post("/coordinator/checkpoints/{checkpoint_id}/approve")
async def approve_checkpoint(checkpoint_id: str, approval: CheckpointApprovalRequest):
    """Approve or reject a checkpoint."""
    # Update checkpoint
    # Resume graph from checkpoint
    pass
```

### Step 7: Initialize Next.js Research Client

```bash
cd ../../  # Go to repo root
mkdir -p research_client
cd research_client
npx create-next-app@latest . --typescript --tailwind --app --no-src-dir
```

```typescript
// research_client/app/page.tsx
export default function Home() {
  return (
    <div>
      <h1>Research Plan Builder</h1>
      {/* Thread list or create new thread */}
    </div>
  );
}

// research_client/app/threads/[thread_id]/page.tsx
export default function ThreadPage({ params }: { params: { thread_id: string } }) {
  return (
    <ChatInterface threadId={params.thread_id} />
  );
}
```

### Step 8: Create .cursor/rules Specification

```markdown
# .cursor/rules/coordinator-agent.mdc
---
description: Coordinator Agent implementation specification
---

# Coordinator Agent Specification

[Full detailed spec with examples, state transitions, tool usage patterns]
```

---

## PHASE 2: Research Plan & Mission Orchestration (Week 2-3)

**Goal**: Redesign research plan structure and enhance execution engine

### Step 1: Redesign Research Plan Models

```python
# models/mongo/research/embedded/research_objectives.py
class ResearchObjective(BaseModel):
    objective_id: str
    description: str  # "Identify the leadership team"
    success_criteria: List[str]
    priority: str  # "critical" | "high" | "medium" | "low"

# models/mongo/research/embedded/agent_instance_plan.py
class AgentInstancePlan(BaseModel):
    instance_id: str
    agent_type: str
    objectives: List[ResearchObjective]  # What to do
    seed_context: Dict[str, Any]
    starter_sources: List[str]  # OPTIONAL
    allowed_tools: List[str]
    requires_outputs_from: List[str]  # Dependencies
    previous_stage_outputs: Optional[Dict]  # From prev stage

# models/mongo/research/embedded/sub_stage.py
class SubStage(BaseModel):
    sub_stage_id: str
    name: str
    agent_instances: List[str]  # instance_ids
    execution_mode: str  # "parallel" | "sequential"
    depends_on_sub_stages: List[str]
    output_aggregation: str  # "merge_all" | "best_of" | "consensus"

# models/mongo/research/embedded/stage.py
class Stage(BaseModel):
    stage_id: str
    name: str
    sub_stages: List[str]  # sub_stage_ids
    execution_mode: str
    depends_on_stages: List[str]

# models/mongo/research/docs/research_mission_plans.py
class ResearchMissionPlanDoc(Document):
    mission_id: str
    created_by: str  # "coordinator_agent"
    mission_name: str
    stages: List[Stage]
    sub_stages: List[SubStage]
    agent_instances: List[AgentInstancePlan]
    execution_strategy: str  # "sequential" | "parallel" | "hybrid"
    # ... (see AGENTS.md for full spec)
```

### Step 2: Update DAG Builder for Flexible Execution

```python
# orchestration/dag/builder.py (ENHANCED)
def build_mission_dag(plan: ResearchMissionPlan) -> MissionDAG:
    """Build DAG from flexible research plan."""
    
    tasks = {}
    
    # Create tasks for agent instances
    for instance in plan.agent_instances:
        task_id = f"instance::{plan.mission_id}::{instance.instance_id}"
        
        # Build dependencies from:
        # 1. requires_outputs_from (instance dependencies)
        # 2. sub_stage dependencies
        # 3. stage dependencies
        depends_on = build_dependencies(instance, plan)
        
        tasks[task_id] = TaskDefinition(
            task_id=task_id,
            task_type="INSTANCE_RUN",
            depends_on=depends_on,
            payload=instance.model_dump(),
        )
    
    # Create aggregation tasks for sub-stages
    for sub_stage in plan.sub_stages:
        task_id = f"substage_reduce::{plan.mission_id}::{sub_stage.sub_stage_id}"
        depends_on = [f"instance::{plan.mission_id}::{inst_id}" 
                      for inst_id in sub_stage.agent_instances]
        
        tasks[task_id] = TaskDefinition(
            task_id=task_id,
            task_type="SUBSTAGE_REDUCE",
            depends_on=depends_on,
            payload=sub_stage.model_dump(),
        )
    
    return MissionDAG(tasks=tasks)
```

### Step 3: Enhance Worker Execution (Output Passing)

```python
# orchestration/workers/handlers/instance_run.py (ENHANCED)
async def handle_instance_run(task: TaskDefinition) -> TaskResult:
    """Execute agent instance with output passing support."""
    
    plan = AgentInstancePlan(**task.payload)
    
    # Load previous stage outputs if this instance depends on others
    previous_outputs = None
    if plan.requires_outputs_from:
        previous_outputs = await load_outputs_from_instances(plan.requires_outputs_from)
    
    # Build agent
    agent = build_worker_agent(
        agent_type=plan.agent_type,
        allowed_tools=plan.allowed_tools,
    )
    
    # Execute with previous outputs
    result = await execute_agent_instance(
        plan=plan,
        previous_outputs=previous_outputs,  # NEW: pass outputs
    )
    
    # Save outputs for downstream instances
    await save_instance_outputs(
        instance_id=plan.instance_id,
        outputs=result.outputs,
    )
    
    return TaskResult(status="completed", outputs=result.outputs)
```

### Step 4: Implement Sub-Stage Aggregation

```python
# orchestration/workers/handlers/substage_reduce.py (NEW)
async def handle_substage_reduce(task: TaskDefinition) -> TaskResult:
    """Aggregate outputs from all instances in a sub-stage."""
    
    sub_stage = SubStage(**task.payload)
    
    # Load outputs from all instances in this sub-stage
    instance_outputs = await load_instance_outputs(sub_stage.agent_instances)
    
    # Aggregate based on strategy
    if sub_stage.output_aggregation == "merge_all":
        aggregated = merge_all_outputs(instance_outputs)
    elif sub_stage.output_aggregation == "best_of":
        aggregated = await llm_select_best_outputs(instance_outputs)
    elif sub_stage.output_aggregation == "consensus":
        aggregated = await llm_find_consensus(instance_outputs)
    
    # Save aggregated outputs
    await save_substage_outputs(
        sub_stage_id=sub_stage.sub_stage_id,
        outputs=aggregated,
    )
    
    return TaskResult(status="completed", outputs=aggregated)
```

### Step 5: Implement WebSocket Progress Streaming

```python
# api/mission_control/websockets/progress_stream.py (NEW)
@router.websocket("/missions/runs/{run_id}/progress")
async def stream_mission_progress(websocket: WebSocket, run_id: str):
    """Stream real-time progress updates."""
    await websocket.accept()
    
    # Subscribe to Redis events for this mission
    async for event in subscribe_to_mission_events(run_id):
        await websocket.send_json({
            "event_type": event.event_type,
            "message": event.message,
            "progress_percent": event.progress_percent,
            "timestamp": event.timestamp.isoformat(),
        })
```

### Step 6: Update Coordinator Agent to Generate Flexible Plans

```python
# graphs/coordinator/nodes/allocate_agents.py (UPDATED)
async def allocate_agents_node(state: CoordinatorAgentState) -> Dict:
    """Allocate agent types with flexible objectives."""
    
    # Generate agent instances with objectives (not domain catalogs)
    agent_instances = []
    for stage in state["research_stages"]:
        for objective_group in stage.objective_groups:
            instance = AgentInstancePlan(
                instance_id=generate_id(),
                agent_type=select_agent_type(objective_group),
                objectives=objective_group.objectives,
                seed_context=build_seed_context(objective_group),
                starter_sources=state.get("source_recommendations", []),  # Optional
                allowed_tools=["tavily_search", "tavily_extract", "write_file", ...],
            )
            agent_instances.append(instance)
    
    return {"agent_instances": agent_instances}
```

### Step 7: Create .cursor/rules Specification

```markdown
# .cursor/rules/research-plan-structure.mdc
---
description: Research Plan flexible structure specification
---

# Research Plan Structure

[Full spec with examples of sequential, parallel, and hybrid execution]
```

---

## PHASE 3: Entity Candidates Refactor (Week 3-4)

**Goal**: Transform entity discovery into lightweight candidate ranking tool

### Step 1: Simplify Entity Discovery Graph

```python
# graphs/candidate_exploration/graph.py (SIMPLIFIED)
def build_candidate_exploration_graph():
    """Build lightweight candidate exploration graph."""
    
    builder = StateGraph(CandidateExplorationState)
    
    # Simplified nodes (NO domain catalogs)
    builder.add_node("quick_extraction", quick_extraction_node)  # Fast LLM extraction
    builder.add_node("relevance_ranking", relevance_ranking_node)  # Score relevance
    builder.add_node("novelty_check", novelty_check_node)  # Query KG
    builder.add_node("completeness_estimate", completeness_estimate_node)  # Quick check
    
    builder.add_edge(START, "quick_extraction")
    builder.add_edge("quick_extraction", "relevance_ranking")
    builder.add_edge("relevance_ranking", "novelty_check")
    builder.add_edge("novelty_check", "completeness_estimate")
    builder.add_edge("completeness_estimate", END)
    
    return builder.compile()
```

### Step 2: Implement Ranking Logic

```python
# graphs/candidate_exploration/nodes/relevance_ranking.py
async def relevance_ranking_node(state: CandidateExplorationState) -> Dict:
    """Rank candidates by relevance to query."""
    
    candidates = state["extracted_candidates"]
    query = state["query"]
    
    # Use LLM to score relevance (0-1)
    ranked = await llm_rank_candidates_by_relevance(
        candidates=candidates,
        query=query,
    )
    
    return {"ranked_candidates": ranked}

# graphs/candidate_exploration/nodes/novelty_check.py
async def novelty_check_node(state: CandidateExplorationState) -> Dict:
    """Check if candidates are already in KG."""
    
    for candidate in state["ranked_candidates"]:
        # Query GraphQL API
        existing = await query_kg_for_entity(candidate.canonical_name)
        
        if existing:
            candidate.novelty_score = 0.1  # Already have it
        else:
            candidate.novelty_score = 0.9  # New entity
    
    return {"ranked_candidates": state["ranked_candidates"]}
```

### Step 3: Create Tool Interface

```python
# tools/candidate_exploration/explore_and_rank.py
class ExploreAndRankCandidatesTool(BaseTool):
    """Tool for Coordinator Agent to explore and rank candidates."""
    
    name = "explore_and_rank_candidates"
    description = """
    Quickly explore candidate entities and rank them by research priority.
    Use this to help users prioritize which entities to research deeply.
    
    Input: query (string), max_candidates (int, default 10)
    Output: Ranked list with recommendations
    """
    
    async def _arun(self, query: str, max_candidates: int = 10) -> str:
        graph = build_candidate_exploration_graph()
        result = await graph.ainvoke({
            "query": query,
            "max_candidates": max_candidates,
        })
        
        # Format for LLM
        output = "Ranked Candidates:\n\n"
        for i, candidate in enumerate(result["ranked_candidates"], 1):
            output += f"{i}. {candidate.canonical_name}\n"
            output += f"   Priority: {candidate.research_priority}\n"
            output += f"   Relevance: {candidate.relevance_score:.2f}\n"
            output += f"   Novelty: {candidate.novelty_score:.2f}\n"
            output += f"   Summary: {candidate.quick_summary}\n\n"
        
        return output
```

### Step 4: Integrate with Coordinator Agent

```python
# graphs/coordinator/graph.py (ADD TOOL)
def build_coordinator_agent_graph():
    """Build Coordinator Agent with candidate exploration tool."""
    
    tools = [
        tavily_search_tool,
        query_existing_entities_tool,
        query_past_research_runs_tool,
        explore_and_rank_candidates_tool,  # NEW TOOL
    ]
    
    # ... rest of graph setup
```

---

## PHASE 4: Extraction & Storage (Week 4+)

**Goal**: Build extraction pipeline to populate knowledge graph

### Step 1: Create Extraction Graph

```python
# graphs/entity_extraction/graph.py
def build_extraction_graph():
    """Build entity extraction graph."""
    
    builder = StateGraph(ExtractionState)
    
    builder.add_node("parse_reports", parse_reports_node)
    builder.add_node("extract_entities", extract_entities_node)
    builder.add_node("extract_relationships", extract_relationships_node)
    builder.add_node("link_evidence", link_evidence_node)
    builder.add_node("graphql_upsert", graphql_upsert_node)
    
    builder.add_edge(START, "parse_reports")
    builder.add_edge("parse_reports", "extract_entities")
    builder.add_edge("extract_entities", "extract_relationships")
    builder.add_edge("extract_relationships", "link_evidence")
    builder.add_edge("link_evidence", "graphql_upsert")
    builder.add_edge("graphql_upsert", END)
    
    return builder.compile()

# graphs/entity_extraction/nodes/extract_entities.py
async def extract_entities_node(state: ExtractionState) -> Dict:
    """Extract structured entities from reports using LLM."""
    
    reports_content = state["parsed_reports"]
    
    # Use LLM with structured outputs
    organizations = await llm_extract_organizations(reports_content)
    people = await llm_extract_people(reports_content)
    products = await llm_extract_products(reports_content)
    compounds = await llm_extract_compounds(reports_content)
    
    return {
        "organizations": organizations,
        "people": people,
        "products": products,
        "compounds": compounds,
    }
```

### Step 2: Implement GraphQL Mutations

```python
# services/graphql/mutations.py
async def upsert_organization(client: GraphQLClient, org: OrganizationExtracted) -> str:
    """Upsert organization to Neo4j."""
    
    mutation = """
    mutation UpsertOrganization($input: OrganizationInput!) {
        upsertOrganization(input: $input) {
            id
            canonicalName
        }
    }
    """
    
    result = await client.execute(mutation, variables={
        "input": {
            "canonicalName": org.canonical_name,
            "domains": org.domains,
            "description": org.description,
            "aliases": org.aliases,
        }
    })
    
    return result["upsertOrganization"]["id"]

# Similar functions for:
# - upsert_person()
# - upsert_product()
# - upsert_compound()
# - create_relationship()
# - link_evidence()
```

### Step 3: Create Extraction API Routes

```python
# api/extraction/routes/extract.py
@router.post("/extraction/extract-from-mission")
async def extract_from_mission(request: ExtractFromMissionRequest) -> ExtractFromMissionResponse:
    """Extract entities from a completed research mission."""
    
    # Load mission outputs
    mission_outputs = await load_mission_outputs(request.mission_id)
    
    # Build extraction graph
    graph = build_extraction_graph()
    
    # Execute extraction
    result = await graph.ainvoke({
        "mission_id": request.mission_id,
        "final_reports": mission_outputs.final_reports,
        "checkpoint_files": mission_outputs.checkpoint_files,
    })
    
    return ExtractFromMissionResponse(
        extraction_run_id=result["extraction_run_id"],
        entities_added=len(result["graphql_entity_ids"]),
        relationships_created=len(result["relationships"]),
    )
```

### Step 4: Integrate with Mission Control

```python
# api/mission_control/routes/missions.py (ENHANCED)
@router.post("/missions/{mission_id}/complete")
async def complete_mission(mission_id: str):
    """Mark mission complete and trigger extraction."""
    
    # Update mission status
    await update_mission_status(mission_id, "completed")
    
    # Trigger extraction (async)
    extraction_task = await trigger_extraction(mission_id)
    
    return {
        "mission_id": mission_id,
        "status": "completed",
        "extraction_run_id": extraction_task.extraction_run_id,
    }
```

### Step 5: Update Research Client UI

```typescript
// research_client/components/mission/MissionProgress.tsx
export function MissionProgress({ missionId }: { missionId: string }) {
  const { progress, isComplete } = useMissionProgress(missionId);
  
  return (
    <div>
      {/* Mission execution progress */}
      <ProgressBar percent={progress.percent} />
      
      {isComplete && (
        <div>
          <h3>Extraction in Progress</h3>
          <ExtractionProgress missionId={missionId} />
        </div>
      )}
    </div>
  );
}
```

---

## Research Client (Next.js Frontend)

The `research_client/` directory is located at the **repository root** (sibling to `ingestion/` and `api/`) and contains the Next.js application for interacting with the Coordinator Agent and monitoring research missions.

**Location**: `C:\Users\Pinda\Proyectos\humanupgradeapp\research_client/`

```
research_client/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ layout.tsx                      # Root layout
â”‚   â”œâ”€â”€ page.tsx                        # Home page (thread list/create)
â”‚   â”œâ”€â”€ threads/
â”‚   â”‚   â”œâ”€â”€ [thread_id]/
â”‚   â”‚   â”‚   â””â”€â”€ page.tsx                # Chat interface with Coordinator
â”‚   â”‚   â””â”€â”€ new/
â”‚   â”‚       â””â”€â”€ page.tsx                # Create new thread
â”‚   â”œâ”€â”€ missions/
â”‚   â”‚   â”œâ”€â”€ [mission_id]/
â”‚   â”‚   â”‚   â””â”€â”€ page.tsx                # Mission execution progress
â”‚   â”‚   â””â”€â”€ list/
â”‚   â”‚       â””â”€â”€ page.tsx                # List all missions
â”‚   â””â”€â”€ api/
â”‚       â””â”€â”€ coordinator/                # API route proxies (optional)
â”‚
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ chat/
â”‚   â”‚   â”œâ”€â”€ ChatInterface.tsx           # Main chat UI
â”‚   â”‚   â”œâ”€â”€ MessageBubble.tsx           # Message display
â”‚   â”‚   â”œâ”€â”€ MessageInput.tsx            # User input
â”‚   â”‚   â”œâ”€â”€ ApprovalCheckpoint.tsx      # Human approval UI
â”‚   â”‚   â””â”€â”€ StreamingIndicator.tsx      # Loading/streaming state
â”‚   â”œâ”€â”€ plan/
â”‚   â”‚   â”œâ”€â”€ ResearchPlanView.tsx        # Full plan visualization
â”‚   â”‚   â”œâ”€â”€ StageCard.tsx               # Stage display
â”‚   â”‚   â”œâ”€â”€ SubStageCard.tsx            # Sub-stage display
â”‚   â”‚   â”œâ”€â”€ AgentAllocationView.tsx     # Agent types per stage
â”‚   â”‚   â””â”€â”€ DependencyGraph.tsx         # Visual dependency graph
â”‚   â”œâ”€â”€ mission/
â”‚   â”‚   â”œâ”€â”€ MissionProgress.tsx         # Real-time progress
â”‚   â”‚   â”œâ”€â”€ StageProgress.tsx           # Per-stage progress
â”‚   â”‚   â”œâ”€â”€ AgentProgress.tsx           # Per-agent progress
â”‚   â”‚   â””â”€â”€ ExtractionProgress.tsx      # Extraction progress
â”‚   â”œâ”€â”€ knowledge-graph/
â”‚   â”‚   â”œâ”€â”€ ExistingEntitiesView.tsx    # Show entities in KG
â”‚   â”‚   â”œâ”€â”€ EntityCard.tsx              # Single entity display
â”‚   â”‚   â””â”€â”€ RelationshipGraph.tsx       # Entity relationships
â”‚   â””â”€â”€ ui/
â”‚       â”œâ”€â”€ Button.tsx                  # shadcn/ui components
â”‚       â”œâ”€â”€ Card.tsx
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ api-client.ts                   # FastAPI client (fetch wrapper)
â”‚   â”œâ”€â”€ websocket.ts                    # WebSocket manager
â”‚   â”œâ”€â”€ types.ts                        # TypeScript types
â”‚   â””â”€â”€ utils.ts                        # Utility functions
â”‚
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ useCoordinatorThread.ts         # Thread state management
â”‚   â”œâ”€â”€ useCheckpointApproval.ts        # Approval flow
â”‚   â”œâ”€â”€ useMissionProgress.ts           # Mission progress subscription
â”‚   â””â”€â”€ useWebSocket.ts                 # WebSocket connection
â”‚
â”œâ”€â”€ package.json                        # Dependencies
â”œâ”€â”€ tsconfig.json                       # TypeScript config
â”œâ”€â”€ tailwind.config.ts                  # Tailwind config
â””â”€â”€ next.config.js                      # Next.js config
```

**Key Features:**
- Real-time chat with Coordinator Agent (WebSocket streaming)
- Human-in-the-loop approval UI (scope + final plan)
- Visual research plan builder/viewer
- Live mission progress tracking
- Knowledge graph entity browser

---

## Project Structure (Updated with Phase Components)

The following directories are added/modified per phase:

### Phase 1 Additions
```
api/coordinator/
graphs/coordinator/
models/mongo/coordinator/
tools/graphql/
tools/research_history/
research_client/  # NEW: Next.js frontend
```

### Phase 2 Modifications
```
models/mongo/research/  # Redesigned models
orchestration/dag/  # Enhanced DAG builder
orchestration/workers/handlers/  # Enhanced handlers
api/mission_control/websockets/  # Progress streaming
```

### Phase 3 Additions
```
graphs/candidate_exploration/  # Simplified from entity_discovery
models/mongo/candidates/  # Simplified models
tools/candidate_exploration/
```

### Phase 4 Additions
```
graphs/entity_extraction/
models/mongo/extraction/
services/graphql/
api/extraction/
```

---

### Phase 5: Graphs (Week 3)

**Goal**: Reorganize LangGraph implementations

1. **Move entity discovery graph**:
   ```bash
   mv human_upgrade/graphs/entity_candidates_connected_graph.py graphs/entity_discovery/graph.py
   
   # Split nodes into separate files:
   # graphs/entity_discovery/nodes/seed_extraction.py
   # graphs/entity_discovery/nodes/official_sources.py
   # graphs/entity_discovery/nodes/domain_catalogs.py
   # graphs/entity_discovery/nodes/candidate_slices.py
   # graphs/entity_discovery/nodes/persistence.py
   
   # Extract state:
   # graphs/entity_discovery/state.py (EntityIntelConnectedCandidatesAndSourcesState)
   
   # Extract helpers:
   # graphs/entity_discovery/helpers.py (_filter_catalogs_for_fanout, etc.)
   ```

2. **Move research plan graph**:
   ```bash
   mv human_upgrade/graphs/research_plan_graph.py graphs/research_planning/graph.py
   
   # Split nodes:
   # graphs/research_planning/nodes/initial_plan.py
   # graphs/research_planning/nodes/source_expansion.py
   # graphs/research_planning/nodes/attach_sources.py
   # graphs/research_planning/nodes/assemble_final.py
   ```

3. **Move agent instance factory**:
   ```bash
   mv human_upgrade/graphs/agent_instance_factory.py agents/factory/
   # Split into: builder.py, runner.py, middleware.py
   ```

4. **Move persistence/checkpointing**:
   ```bash
   mv human_upgrade/persistence/checkpointer_and_store.py memory/store/checkpointer.py
   ```

5. **Update graph imports**:
   ```python
   # Before:
   from research_agent.human_upgrade.graphs.entity_candidates_connected_graph import (
       make_entity_intel_connected_candidates_and_sources_graph
   )
   
   # After:
   from research_agent.graphs.entity_discovery.graph import (
       make_entity_intel_connected_candidates_and_sources_graph
   )
   ```

### Phase 6: Agents (Week 3-4)

**Goal**: Organize worker agent implementations

1. **Create agent factory**:
   ```bash
   # agents/factory/builder.py (build_worker_agent function)
   # agents/factory/runner.py (run_worker_once function)
   # agents/factory/middleware.py (dynamic_prompt, summarizer, after_agent)
   ```

2. **Move agent state**:
   ```bash
   mv human_upgrade/graphs/state/agent_instance_state.py agents/state/worker_agent_state.py
   ```

3. **Create agent type implementations** (optional for now):
   ```python
   # agents/types/business_identity.py
   # Future: Agent-specific configuration overrides
   ```

4. **Move tool configuration**:
   ```bash
   mv human_upgrade/utils/default_tools_by_agent_type.py agents/tools/default_tool_maps.py
   mv human_upgrade/utils/research_tools_map.py agents/tools/tool_registry.py
   ```

### Phase 7: Orchestration (Week 4)

**Goal**: Organize mission DAG + scheduler + workers

1. **Rename mission_queue â†’ orchestration**:
   ```bash
   mv mission_queue/ orchestration/
   mv orchestration/mission_dag_builder.py orchestration/dag/builder.py
   mv orchestration/schemas.py orchestration/dag/schemas.py
   mv orchestration/scheduler_in_memory.py orchestration/scheduler/in_memory.py
   mv orchestration/worker.py orchestration/workers/taskiq_worker.py
   ```

2. **Split worker handlers**:
   ```python
   # orchestration/workers/handlers/instance_run.py (handle_instance_run)
   # orchestration/workers/handlers/substage_reduce.py (handle_substage_reduce)
   ```

3. **Move Taskiq broker**:
   ```bash
   mv taskiq_tests/setup.py orchestration/queue/taskiq_broker.py
   # Clean up taskiq_tests/ (keep only test files)
   ```

### Phase 8: Services (Week 4-5)

**Goal**: Ensure MongoDB services are well-organized

1. **Verify services/ structure** (already good):
   ```
   services/
   â”œâ”€â”€ mongo/
   â”‚   â”œâ”€â”€ candidates/
   â”‚   â”œâ”€â”€ research/
   â”‚   â””â”€â”€ common/
   ```

2. **Add Neo4j services** (future):
   ```python
   # services/neo4j/entity_ingestion_service.py
   # services/neo4j/relationship_service.py
   ```

3. **Add GraphQL client service**:
   ```python
   # services/graphql/client.py (Ariadne-generated client)
   # services/graphql/mutations.py (Common mutation builders)
   ```

### Phase 9: Memory (Week 5)

**Goal**: Organize LangMem + LangGraph Store

1. **Move memory module**:
   ```bash
   mv human_upgrade/graphs/memory/ memory/langmem/
   # Rename langmem_manager.py â†’ manager.py
   # Rename langmem_schemas.py â†’ schemas.py
   # Rename langmem_namespaces.py â†’ namespaces.py
   ```

2. **Move store utilities**:
   ```bash
   # memory/store/checkpointer.py (from human_upgrade/persistence/)
   # memory/store/postgres_store.py (new: AsyncPostgresStore config)
   # memory/store/recall.py (memory recall utilities)
   ```

3. **Create memory tools** (LangChain tools for agent memory recall):
   ```python
   # memory/tools/memory_tools.py
   from langchain.tools import BaseTool
   from ..langmem.manager import recall_semantic_for_org
   
   class RecallSemanticMemoryTool(BaseTool):
       name = "recall_semantic_memory"
       description = "Recall semantic memories for an entity"
       
       async def _arun(self, entity_id: str) -> str:
           memories = await recall_semantic_for_org(entity_id)
           return str(memories)
   ```

### Phase 10: API Servers (Week 5-6)

**Goal**: Build out FastAPI server layer

1. **Create Graph Execution API**:
   ```python
   # api/graph_execution/main.py
   from fastapi import FastAPI
   from .routes import entity_discovery, research_planning, health
   
   app = FastAPI(title="Graph Execution API")
   app.include_router(entity_discovery.router)
   app.include_router(research_planning.router)
   app.include_router(health.router)
   ```

2. **Create Mission Control API**:
   ```python
   # api/mission_control/main.py
   from fastapi import FastAPI
   from .routes import missions, runs, tasks, plans
   
   app = FastAPI(title="Mission Control API")
   app.include_router(missions.router)
   app.include_router(runs.router)
   app.include_router(tasks.router)
   app.include_router(plans.router)
   ```

3. **Create Memory & Thread API**:
   ```python
   # api/memory_and_threads/main.py
   from fastapi import FastAPI
   from .routes import threads, checkpoints, memory, recall
   
   app = FastAPI(title="Memory & Thread API")
   app.include_router(threads.router)
   app.include_router(checkpoints.router)
   app.include_router(memory.router)
   app.include_router(recall.router)
   ```

4. **Create MongoDB Model API**:
   ```python
   # api/mongodb_models/main.py
   from fastapi import FastAPI
   from .routes import research_plans, research_runs, candidates, entities
   
   app = FastAPI(title="MongoDB Model API")
   app.include_router(research_plans.router)
   app.include_router(research_runs.router)
   app.include_router(candidates.router)
   app.include_router(entities.router)
   ```

5. **Implement routes** (iteratively per server)

### Phase 11: Configuration & Testing (Week 6-7)

**Goal**: Add configuration management + comprehensive tests

1. **Create settings module**:
   ```python
   # config/settings.py
   from pydantic_settings import BaseSettings
   
   class Settings(BaseSettings):
       MONGO_URI: str
       MONGO_BIOTECH_DB_NAME: str
       REDIS_URL: str
       RABBITMQ_URL: str
       NEO4J_URI: str
       OPENAI_API_KEY: str
       # ... all env vars
       
       class Config:
           env_file = ".env"
   ```

2. **Write unit tests**:
   ```python
   # tests/unit/graphs/test_entity_discovery_nodes.py
   # tests/unit/agents/test_worker_agent_factory.py
   # tests/unit/services/test_mongo_services.py
   ```

3. **Write integration tests**:
   ```python
   # tests/integration/test_entity_discovery_graph.py
   # tests/integration/test_research_plan_graph.py
   # tests/integration/test_mission_dag_execution.py
   ```

4. **Create test fixtures**:
   ```python
   # tests/fixtures/mongo_fixtures.py (Beanie test fixtures)
   # tests/fixtures/graph_fixtures.py (Mock graph states)
   ```

### Phase 12: Cleanup & Documentation (Week 7)

**Goal**: Delete human_upgrade/, update all docs

1. **Verify all imports updated**:
   ```bash
   # Search for any remaining human_upgrade imports
   grep -r "from research_agent.human_upgrade" .
   grep -r "import research_agent.human_upgrade" .
   ```

2. **Delete legacy module**:
   ```bash
   rm -rf human_upgrade/
   ```

3. **Update README.md**:
   - Document new structure
   - Update import examples
   - Add setup instructions

4. **Update AGENTS.md** (reflect new structure)

5. **Create migration guide** (MIGRATION_GUIDE.md):
   - Old imports â†’ new imports mapping
   - Key architectural changes
   - Breaking changes (if any)

---

## Import Path Changes (Reference)

### Before â†’ After Mapping

| **Before** | **After** |
|------------|-----------|
| `research_agent.human_upgrade.graphs.entity_candidates_connected_graph` | `research_agent.graphs.entity_discovery.graph` |
| `research_agent.human_upgrade.graphs.research_plan_graph` | `research_agent.graphs.research_planning.graph` |
| `research_agent.human_upgrade.graphs.agent_instance_factory` | `research_agent.agents.factory.builder` |
| `research_agent.human_upgrade.structured_outputs.candidates_outputs` | `research_agent.models.graph.candidates` |
| `research_agent.human_upgrade.structured_outputs.research_plans_outputs` | `research_agent.models.graph.research_plans` |
| `research_agent.human_upgrade.tools.web_search_tools` | `research_agent.tools.web` (split) |
| `research_agent.human_upgrade.prompts.candidates_prompts` | `research_agent.prompts.graphs.entity_discovery` (split) |
| `research_agent.human_upgrade.prompts.sub_agent_prompt_builders` | `research_agent.agents.prompts.initial` |
| `research_agent.human_upgrade.utils.artifacts` | `research_agent.shared.artifacts` |
| `research_agent.human_upgrade.base_models` | `research_agent.infrastructure.llm.model_registry` |
| `research_agent.human_upgrade.logger` | `research_agent.shared.logging_utils` |
| `research_agent.human_upgrade.graphs.memory.langmem_manager` | `research_agent.memory.langmem.manager` |
| `research_agent.human_upgrade.persistence.checkpointer_and_store` | `research_agent.memory.store.checkpointer` |
| `research_agent.mission_queue.mission_dag_builder` | `research_agent.orchestration.dag.builder` |
| `research_agent.mission_queue.scheduler_in_memory` | `research_agent.orchestration.scheduler.in_memory` |
| `research_agent.mission_queue.worker` | `research_agent.orchestration.workers.taskiq_worker` |

---

## Testing Strategy

### Unit Tests (Fast, isolated)

```python
# tests/unit/graphs/test_entity_discovery_nodes.py
import pytest
from research_agent.graphs.entity_discovery.nodes import seed_extraction

@pytest.mark.asyncio
async def test_seed_extraction_node():
    state = {"query": "Research Ozempic", "starter_sources": []}
    result = await seed_extraction.seed_extraction_node(state)
    assert "seed_extraction" in result
    assert result["seed_extraction"].organization_candidates
```

### Integration Tests (Slower, full workflows)

```python
# tests/integration/test_entity_discovery_graph.py
import pytest
from research_agent.graphs.entity_discovery.graph import (
    make_entity_intel_connected_candidates_and_sources_graph
)

@pytest.mark.asyncio
async def test_full_entity_discovery_workflow():
    graph = await make_entity_intel_connected_candidates_and_sources_graph({})
    result = await graph.ainvoke({
        "query": "Research Ozempic",
        "starter_sources": ["https://www.novonordisk.com"],
        "starter_content": "",
    })
    assert "candidate_sources" in result
    assert result["candidate_sources"].connected
```

### Fixture Strategy

```python
# tests/fixtures/mongo_fixtures.py
import pytest
from motor.motor_asyncio import AsyncIOMotorClient
from beanie import init_beanie
from research_agent.models.mongo import get_document_models

@pytest.fixture(scope="session")
async def mongo_test_db():
    client = AsyncIOMotorClient("mongodb://localhost:27017")
    db = client["test_biotech_research_db"]
    await init_beanie(database=db, document_models=get_document_models())
    yield db
    await client.drop_database("test_biotech_research_db")
    client.close()
```

---

## Server Deployment Strategy

### Development (Local)

```bash
# Terminal 1: Graph Execution API
uvicorn research_agent.api.graph_execution.main:app --reload --port 8001

# Terminal 2: Mission Control API
uvicorn research_agent.api.mission_control.main:app --reload --port 8002

# Terminal 3: Memory & Thread API
uvicorn research_agent.api.memory_and_threads.main:app --reload --port 8003

# Terminal 4: MongoDB Model API
uvicorn research_agent.api.mongodb_models.main:app --reload --port 8004

# Terminal 5: Scheduler
python -m research_agent.orchestration.scheduler.in_memory

# Terminal 6-N: Workers (scale as needed)
python -m research_agent.orchestration.workers.taskiq_worker
```

### Production (Docker Compose)

```yaml
# docker-compose.yml
version: '3.8'

services:
  graph-execution-api:
    build: .
    command: uvicorn research_agent.api.graph_execution.main:app --host 0.0.0.0 --port 8001
    ports:
      - "8001:8001"
    environment:
      - MONGO_URI=${MONGO_URI}
      - REDIS_URL=${REDIS_URL}
    depends_on:
      - mongo
      - redis
  
  mission-control-api:
    build: .
    command: uvicorn research_agent.api.mission_control.main:app --host 0.0.0.0 --port 8002
    ports:
      - "8002:8002"
    depends_on:
      - mongo
      - redis
  
  scheduler:
    build: .
    command: python -m research_agent.orchestration.scheduler.in_memory
    depends_on:
      - redis
      - rabbitmq
  
  worker:
    build: .
    command: python -m research_agent.orchestration.workers.taskiq_worker
    deploy:
      replicas: 4
    depends_on:
      - redis
      - rabbitmq
      - mongo
  
  mongo:
    image: mongo:7
    ports:
      - "27017:27017"
  
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
  
  rabbitmq:
    image: rabbitmq:3-management
    ports:
      - "5672:5672"
      - "15672:15672"
```

---

## Success Criteria by Phase

### Phase 1: Coordinator Agent âœ… When:
1. âœ… User can start conversation with Coordinator Agent via Next.js client
2. âœ… Coordinator can query existing KG entities (GraphQL tools working)
3. âœ… Coordinator can query past research runs
4. âœ… Coordinator can build research plans conversationally
5. âœ… Scope approval checkpoint works (human can approve/reject)
6. âœ… Final plan approval checkpoint works
7. âœ… Approved plans saved to MongoDB
8. âœ… `.cursor/rules/coordinator-agent.mdc` specification created

### Phase 2: Research Plan & Orchestration âœ… When:
1. âœ… Flexible research plan models implemented (no domain catalog dependency)
2. âœ… Agents execute with objectives + optional starter sources
3. âœ… DAG builder supports flexible dependencies (instance â†’ instance, sub-stage â†’ stage)
4. âœ… Output passing works (agents can receive previous_outputs)
5. âœ… Sub-stage aggregation works (merge_all, best_of, consensus)
6. âœ… WebSocket progress streams to research client
7. âœ… Missions complete with correct outputs passed between stages
8. âœ… `.cursor/rules/research-plan-structure.mdc` specification created

### Phase 3: Candidate Exploration âœ… When:
1. âœ… Simplified exploration graph completes in <30 seconds
2. âœ… Candidates ranked by relevance, novelty, completeness
3. âœ… Tool integrated with Coordinator Agent
4. âœ… Coordinator can recommend prioritized candidates to users
5. âœ… No dependency on domain catalog generation

### Phase 4: Extraction & Storage âœ… When:
1. âœ… Extraction graph processes research outputs successfully
2. âœ… Entities extracted using LLM structured outputs
3. âœ… GraphQL mutations working (entities created in Neo4j)
4. âœ… Relationships created correctly
5. âœ… Evidence linked to entities
6. âœ… Knowledge graph grows with each research mission
7. âœ… Extraction progress visible in research client

### Overall System âœ… When:
1. âœ… End-to-end flow works: Coordinator â†’ Plan â†’ Execute â†’ Extract â†’ KG
2. âœ… All 4 FastAPI servers running (Coordinator, Mission Control, Memory, Extraction)
3. âœ… Next.js research client functional
4. âœ… Human-in-the-loop flow smooth (scope + final approval)
5. âœ… WebSocket progress streaming works end-to-end
6. âœ… Tests passing (unit + integration per phase)
7. âœ… Documentation complete (AGENTS.md, .cursor/rules/*.mdc files)
8. âœ… Research system is flexible (not rigid domain-catalog-based)

---

## Benefits of New Structure

### 1. Clear Separation of Concerns
- **API Layer**: FastAPI servers (external interface)
- **Graph Layer**: LangGraph orchestration (research logic)
- **Agent Layer**: Worker agents (execution)
- **Service Layer**: MongoDB + Neo4j operations (persistence)
- **Infrastructure Layer**: External integrations (storage, queue, LLMs)

### 2. Independent Server Scaling
- Graph Execution API: Scale for concurrent graph runs
- Mission Control API: Scale for mission orchestration
- Workers: Scale for parallel agent execution
- Each server has clear responsibility

### 3. Easier Testing
- Unit tests per layer (isolated)
- Integration tests per workflow
- Clear fixture strategy (MongoDB, Redis, RabbitMQ)

### 4. Better Developer Experience
- Intuitive directory names (graphs, agents, services)
- Logical grouping (web tools, filesystem tools, scholarly tools)
- Centralized registries (tool registry, model registry)
- Clear import paths (research_agent.graphs.entity_discovery.graph)

### 5. Future-Proof Architecture
- Easy to add new agent types (agents/types/)
- Easy to add new graphs (graphs/new_graph/)
- Easy to add new API servers (api/new_server/)
- Easy to swap implementations (e.g., in_memory scheduler â†’ mongo_backed scheduler)

### 6. Maintainability
- Single source of truth for models (models/)
- Single source of truth for prompts (prompts/)
- Single source of truth for tools (tools/)
- Clear deprecation path (delete old module when ready)

---

## Migration Risks & Mitigation

### Risk 1: Import Hell (High Likelihood)

**Risk**: Updating hundreds of imports across the codebase
**Mitigation**:
- Automated find-replace scripts
- Incremental migration (phase by phase)
- Keep both paths working temporarily (deprecated imports)
- Comprehensive test suite to catch broken imports

### Risk 2: Circular Dependencies (Medium Likelihood)

**Risk**: New structure introduces circular imports
**Mitigation**:
- Careful layer design (API â†’ Graphs â†’ Agents â†’ Services)
- Use dependency injection where needed
- Lazy imports (`from typing import TYPE_CHECKING`)
- Clear interface definitions

### Risk 3: Breaking Production (Low Likelihood)

**Risk**: Migration breaks existing production workflows
**Mitigation**:
- Feature flag new structure (run both old + new in parallel)
- Comprehensive integration tests before cutover
- Gradual rollout (internal testing â†’ staging â†’ production)
- Rollback plan (Git branch + Docker image)

### Risk 4: Lost Context (Medium Likelihood)

**Risk**: Team loses familiarity with codebase during migration
**Mitigation**:
- Detailed MIGRATION_GUIDE.md (import path mapping)
- Pair programming during migration
- Code review every phase
- Update AGENTS.md incrementally

---

## Implementation Timeline

**Target**: 3-4 weeks for full MVP

| **Phase** | **Duration** | **Key Deliverables** | **Priority** |
|-----------|--------------|----------------------|--------------|
| **Phase 1: Coordinator Agent** | Week 1 | MongoDB models, GraphQL tools, Coordinator graph, FastAPI routes, Next.js client, Human-in-the-loop checkpoints | **HIGHEST** |
| **Phase 2: Research Plan & Orchestration** | Week 2-3 | Flexible plan models, Enhanced DAG builder, Output passing, WebSocket progress, Sub-stage aggregation | **CRITICAL** |
| **Phase 3: Candidate Exploration** | Week 3-4 | Simplified exploration graph, Ranking logic, Tool integration, Coordinator integration | **HIGH** |
| **Phase 4: Extraction & Storage** | Week 4+ | Extraction graph, GraphQL mutations, Extraction API, KG population | **MEDIUM** |

**Development Strategy**: Build new components first (Phases 1-4), then refactor existing code to integrate.

---

## Immediate Next Steps (Phase 1 - This Week)

### Day 1-2: MongoDB Models + Tools
1. âœ… Create `CoordinatorThreadDoc` and `CoordinatorCheckpointDoc` Beanie models
2. âœ… Implement GraphQL query tools (`query_existing_entities`, `get_entity_details`)
3. âœ… Implement research history tools (`query_past_runs`, `get_run_summary`)
4. âœ… Write unit tests for models and tools

### Day 3-4: Coordinator LangGraph
1. âœ… Create `CoordinatorAgentState` TypedDict
2. âœ… Implement coordinator graph nodes (understand_goals, query_knowledge, etc.)
3. âœ… Implement human-in-the-loop checkpoints (`interrupt()`)
4. âœ… Write integration tests for graph flow
5. âœ… Create `.cursor/rules/coordinator-agent.mdc` specification

### Day 5-6: FastAPI Routes + Next.js Client
1. âœ… Create FastAPI Coordinator routes (threads, messages, checkpoints)
2. âœ… Implement WebSocket streaming
3. âœ… Initialize Next.js `research_client/` app
4. âœ… Build basic chat interface component
5. âœ… Build approval checkpoint UI component

### Day 7: Testing + Integration
1. âœ… E2E test: Create thread â†’ chat â†’ approve scope â†’ approve plan
2. âœ… Test WebSocket streaming
3. âœ… Test checkpoint approval flow
4. âœ… Documentation review

**Phase 1 Complete** â†’ Move to Phase 2

---

## Development Principles

1. **Build New First**: Create new components (Phases 1-4) before refactoring old code
2. **Document as We Go**: Create `.cursor/rules/*.mdc` files for each major component
3. **Test Early**: Write tests alongside implementation
4. **Iterate Based on Feedback**: Human-in-the-loop means iterating on UX
5. **Flexible Over Perfect**: Build for flexibility first, optimize later

---

**Last Updated**: 2026-02-15  
**Version**: 2.0 (NEW PHASED ARCHITECTURE)  
**Status**: Living Document - Active Development Phase 1  
**Next Review**: After Phase 1 Complete
